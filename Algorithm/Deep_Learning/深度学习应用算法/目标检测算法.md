# 第二部分：目标检测算法

## 1、简介

目标检测\(object detection\)是计算机视觉中非常具有挑战性的一项工作，一方面它是其他很多后续视觉任务的基础，另一方面目标检测不仅需要预测区域，还要进行分类，因此问题更加复杂。

**目标检测算法可以分为两条主线**

* 基于候选区\(region proposal\)的方法，即通过某种策略选出一部分候选框再进行后续处理，比如RCNN-SPP-Fast RCNN-Faster RCNN-RFCN等
* 端对端的方法，直接使用卷积神经网络将图像变换到一个小的尺度，在新的尺度上使用anchor box来预测目标区域，这一类方法包括SSD以及Yolo的各个版本等

## 2、目标检测的评价指标

准确率 （Accuracy），混淆矩阵 （Confusion Matrix），精确率（Precision），召回率（Recall），平均正确率（AP），mean Average Precision\(mAP\)，交除并（IoU），ROC + AUC，非极大值抑制（NMS）。

### 2.1、**混淆矩阵 （Confusion Matrix）**

我们通过样本的采集，能够直接知道真实情况下，哪些数据结果是positive，哪些结果是negative。同时，我们通过用样本数据跑出分类型模型的结果，也可以知道模型认为这些数据哪些是（正样本）positive，哪些是（负样本）negative。

![](/Image/算法/深度学习/深度学习应用算法/混淆矩阵.jpg)

#### 2.1.1、一级指标

因此，我们就能得到这样四个基础指标，我称他们是一级指标（最底层的）：

* 真实值是positive，模型认为是positive的数量（True Positive=TP）
* 真实值是positive，模型认为是negative的数量（False Negative=FN）：这就是统计学上的第一类错误（Type I Error）
* 真实值是negative，模型认为是positive的数量（False Positive=FP）：这就是统计学上的第二类错误（Type II Error）
* 真实值是negative，模型认为是negative的数量（True Negative=TN）

预测性分类模型，肯定是希望越准越好。那么，对应到混淆矩阵中，那肯定是希望TP与TN的数量大，而FP与FN的数量小。所以当我们得到了模型的混淆矩阵后，就需要去看有多少观测值在第二、四象限对应的位置，这里的数值越多越好；反之，在第一、三四象限对应位置出现的观测值肯定是越少越好。

#### 2.1.2、二级指标

当面对大量的数据，光凭算个数，很难衡量模型的优劣。因此混淆矩阵在基本的统计结果上又延伸了如下4个指标，我称他们是二级指标（通过最底层指标加减乘除得到的）：

* 准确率（Accuracy）—— 针对整个模型
* 精确率（Precision）
* 灵敏度（Sensitivity）：就是召回率（Recall）
* 特异度（Specificity）

![](/Image/算法/深度学习/深度学习应用算法/二级指标.jpg)

通过上面的四个二级指标，可以将混淆矩阵中数量的结果转化为0-1之间的比率。便于进行标准化的衡量。

#### 2.1.3、三级指标

在这四个指标的基础上在进行拓展，会产令另外一个三级指标这个，指标叫做F1 Score。他的计算公式是：


$$
F = \frac{（1+\alpha）PR}{\alpha（P+R）}
$$


当$$\alpha$$为1时即为F1参数，上述公式中，P为Precision，R为Recall。

### 2.2、**平均精度（Average-Precision，AP）与** **mean Average Precision\(mAP\)**

* **AP**就是**Precision-recall 曲线下面的面积**，通常来说一个越好的分类器，AP值越高。
* **mAP是多个类别AP的平均值。**这个mean的意思是对每个类的AP再求平均，得到的就是mAP的值，mAP的大小一定在\[0,1\]区间，越大越好。该指标是目标检测算法中最重要的一个。
  **在正样本非常少的情况下，PR表现的效果会更好。**

![](/Image/算法/深度学习/深度学习应用算法/PR曲线.png)

### 2.3、**IoU**\(交并比\)

IoU这一值，可以理解为系统预测出来的框与原来图片中标记的框的重合程度。 计算方法即检测结果Detection Result与 Ground Truth （真实物体包含的区域）的交集比上它们的并集，即为检测的准确率。

**IOU正是表达这种bounding box和groundtruth的差异的指标：**


$$
IOU =  \frac{DetectionResult\cap GroundTruth}{DetectionResult\cup GroundTruth}
$$


### 2.4、**ROC（Receiver Operating Characteristic）曲线与AUC（Area Under Curve）**

### 2.5、**PR曲线和ROC曲线比较**

### 2.6、**非极大值抑制（NMS）**

**非极大值抑制就是把不是极大值的抑制掉，在物体检测上，就是对一个目标有多个标定框，使用极大值抑制算法滤掉多余的标定框。**

Non-Maximum Suppression就是需要根据score矩阵和region的坐标信息，从中找到置信度比较高的bounding box。对于有重叠在一起的预测框，只保留得分最高的那个。

1. NMS计算出每一个bounding box的面积，然后根据score进行排序，把score最大的bounding box作为队列中首个要比较的对象；
2. 计算其余bounding box与当前最大score与box的IoU，去除IoU大于设定的阈值的bounding box，保留小的IoU得预测框；
3. 然后重复上面的过程，直至候选bounding box为空。

最终，检测了bounding box的过程中有两个阈值，一个就是IoU，另一个是在过程之后，从候选的bounding box中剔除score小于阈值的bounding box。需要注意的是：Non-Maximum Suppression**一次处理一个类别**，如果有N个类别，Non-Maximum Suppression就需要执行N次。

## 3、目标检测概述

**目标检测的工作为：**判断某个固定的像素区域中是否包含物体，并且判断其中的物体属于哪一类，我们把框住这个区域的矩形框叫做bounding box，那么目标检测的问题可以看成是：给定目标类别集合比如{airplane,bird,motorbike,person,sofa}，判断这些bounding box中是否包含目标，

**目标检测输出：**对于这些预测的bounding box是否真实包含物体，通常会有预测的置信度，也就是这个框出来的区域有多大概率属于某类的物体，所以目标检测实际的输出会包含三个内容：预测的bounding box，bounding box中物体的类别，以及属于这个类别的置信度。

## 4、目标检测发展和分类

近几年来，目标检测算法取得了很大的突破。比较流行的算法可以分为两类

* 基于Region Proposal的R-CNN系算法,是two-stage的,需要先使用启发式方法（selective search）或者CNN网络（RPN）产生Region Proposal，然后再在Region Proposal上做分类与回归

  * R-CNN
  * Fast R-CNN
  * Faster R-CNN
  * Mask R-CNN

* one-stage算法，其仅仅使用一个CNN网络直接预测不同目标的类别与位置

  * Yolo V1
  * SSD
  * Yolo V2
  * Yolo V3

  **对比：第一类方法是准确度高一些，但是速度慢，但是第二类算法是速度快，但是准确性要低一些**

  ![](/Image/算法/深度学习/深度学习应用算法/检测算法对比.jpg)

  ### 4.1、滑动窗口技术

采用滑动窗口的目标检测算法思路非常简单，它将检测问题转化为了图像分类问题。其基本原理就是采用不同大小和比例（宽高比）的窗口在整张图片上以一定的步长进行滑动，然后对这些窗口对应的区域做图像分类，这样就可以实现对整张图片的检测了，如下图所示，如DPM就是采用这种思路。但是这个方法有致命的缺点，就是你并不知道要检测的目标大小是什么规模，所以你要设置不同大小和比例的窗口去滑动，而且还要选取合适的步长。但是这样会产生很多的子区域，并且都要经过分类器去做预测，这需要很大的计算量，所以你的分类器不能太复杂，因为要保证速度。解决思路之一就是减少要分类的子区域，这就是R-CNN的一个改进策略，其采用了selective search方法来找到最有可能包含目标的子区域（Region Proposal），其实可以看成采用启发式方法过滤掉很多子区域，这会提升效率。

![](/Image/算法/深度学习/深度学习应用算法/滑动窗口技术.jpg)

​    如果你使用的是CNN分类器，那么滑动窗口是非常耗时的。但是结合卷积运算的特点，我们可以使用CNN实现更高效的滑动窗口方法。这里要介绍的是一种全卷积的方法，简单来说就是网络中用卷积层代替了全连接层，如图4所示。输入图片大小是16x16，经过一系列卷积操作，提取了2x2的特征图，但是这个2x2的图上每个元素都是和原图是一一对应的，如图上蓝色的格子对应蓝色的区域，这不就是相当于在原图上做大小为14x14的窗口滑动，且步长为2，共产生4个字区域。最终输出的通道数为4，可以看成4个类别的预测概率值，这样一次CNN计算就可以实现窗口滑动的所有子区域的分类预测。这其实是overfeat算法的思路。之所可以CNN可以实现这样的效果是因为卷积操作的特性，就是图片的空间位置信息的不变性，尽管卷积过程中图片大小减少，但是位置对应关系还是保存的。说点题外话，这个思路也被R-CNN借鉴，从而诞生了Fast R-cNN算法。

![](/Image/算法/深度学习/深度学习应用算法/滑动窗口的CNN实现.jpg)

