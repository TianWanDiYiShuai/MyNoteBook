# 马尔科夫决策过程

&emsp;&emsp;强化学习的学习过程是动态的、 不断交互的过程，所需要的数据也是通过与环境不断交互所产⽣的。所 以，与监督学习和⾮监督学习相⽐，强化学习涉及的对象更多，⽐如动 作，环境，状态转移概率和回报函数等。另外，深度学习如图像识别和语⾳识别解决 的是感知的问题，强化学习解决的是决策的问题。⼈⼯智能的终极⽬的是通过感知进⾏智能决策。

## 1、⻢尔科夫决策过程理论

### 1.1、⻢尔科夫性

&emsp;&emsp;所谓⻢尔科夫性是指系统的下⼀个状态st+1 仅与当前状态st 有关，⽽与以前的状态⽆关。**即对于未来的决策只与现在的状态有关，与过去无关** 。

&emsp;&emsp;定义：状态st 是⻢尔科夫的，当且仅当P[st+1 |st ]=P[st+1 |s1 ，…，st ]。 

&emsp;&emsp;定义中可以看到，当前状态st 其实是蕴含了所有相关的历史信息s1 ， …，st ，⼀旦当前状态已知，历史信息将会被抛弃。 

- **马尔科夫随机过程**

  数学中⽤来描述随机变量序列的学科叫随机过程。所谓随机过 

  程就是指随机变量序列。若随机变量序列中的每个状态都是⻢尔科夫的， 

  则称此随机过程为⻢尔科夫随机过程。

### 1.2、⻢尔科夫过程

&emsp;&emsp;⻢尔科夫过程是⼀个⼆元组（S，P），且满 ⾜：S是有限状态集合，P是状态转移概率。状态转移概率矩阵为： 
$$
P = \begin{bmatrix}
P_{11} & ... & P_{1n} \\ 
...& ... & ...\\ 
P_{n1} & ... & P_{nm} 
\end{bmatrix}
$$

- **马尔科夫链**

  &emsp;&emsp;具备马尔科夫性的状态，从一个状态转移到另一个状态形成的链式结构为马尔科夫链

  **不管是游戏还是机器⼈，他们都是通过动作与环境进⾏交互，并从环境中获得奖励，⽽⻢尔科夫过程中不存在动作和奖励。将动作（策略）和回报考虑在内的⻢尔科夫过程称为⻢尔科夫决策过程。**

### 1.3、⻢尔科夫决策过程

&emsp;&emsp;⻢尔科夫决策过程由元组（S，A，P，R，γ）描述，其中： 

- S 为有限的状态集
- A 为有限的动作集
- P 为状态转移概率 
- R 为回报函数 
- γ 为折扣因⼦，⽤来计算累积回报。 

注意，跟⻢尔科夫过程不同的是，⻢尔科夫决策过程的状态转移概率 

是包含动作的，即:`P_{{ss}'}^a = P[S_{t+1}={s}'|S_{t}=s,A_{t}=a]`



