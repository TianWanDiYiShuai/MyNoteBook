# 马尔科夫决策过程

&emsp;&emsp;强化学习的学习过程是动态的、 不断交互的过程，所需要的数据也是通过与环境不断交互所产⽣的。所 以，与监督学习和⾮监督学习相⽐，强化学习涉及的对象更多，⽐如动 作，环境，状态转移概率和回报函数等。另外，深度学习如图像识别和语⾳识别解决 的是感知的问题，强化学习解决的是决策的问题。⼈⼯智能的终极⽬的是通过感知进⾏智能决策。

## 1、⻢尔科夫决策过程理论

### 1.1、⻢尔科夫性

&emsp;&emsp;所谓⻢尔科夫性是指系统的下⼀个状态st+1 仅与当前状态st 有关，⽽与以前的状态⽆关。**即对于未来的决策只与现在的状态有关，与过去无关** 。

&emsp;&emsp;定义：状态st 是⻢尔科夫的，当且仅当P[st+1 |st ]=P[st+1 |s1 ，…，st ]。 

&emsp;&emsp;定义中可以看到，当前状态st 其实是蕴含了所有相关的历史信息s1 ， …，st ，⼀旦当前状态已知，历史信息将会被抛弃。 

- **马尔科夫随机过程**

  数学中⽤来描述随机变量序列的学科叫随机过程。所谓随机过 

  程就是指随机变量序列。若随机变量序列中的每个状态都是⻢尔科夫的， 

  则称此随机过程为⻢尔科夫随机过程。

### 1.2、⻢尔科夫过程

&emsp;&emsp;⻢尔科夫过程是⼀个⼆元组（S，P），且满 ⾜：S是有限状态集合，P是状态转移概率。状态转移概率矩阵为： 
$$
P = \begin{bmatrix}
P_{11} & ... & P_{1n} \\ 
...& ... & ...\\ 
P_{n1} & ... & P_{nm} 
\end{bmatrix}
$$

- **马尔科夫链**

  &emsp;&emsp;具备马尔科夫性的状态，从一个状态转移到另一个状态形成的链式结构为马尔科夫链

  **不管是游戏还是机器⼈，他们都是通过动作与环境进⾏交互，并从环境中获得奖励，⽽⻢尔科夫过程中不存在动作和奖励。将动作（策略）和回报考虑在内的⻢尔科夫过程称为⻢尔科夫决策过程。**

### 1.3、⻢尔科夫决策过程

&emsp;&emsp;⻢尔科夫决策过程由元组（S，A，P，R，γ）描述，其中： 

- S 为有限的状态集
- A 为有限的动作集
- P 为状态转移概率 
- R 为回报函数 
- γ 为折扣因⼦，⽤来计算累积回报。 

注意，跟⻢尔科夫过程不同的是，⻢尔科夫决策过程的状态转移概率 

是包含动作的，即:
$$
P_{{ss}'}^a = P[S_{t+1}={s}'|S_{t}=s,A_{t}=a]
$$
&emsp;&emsp;强化学习的⽬标是给定⼀个⻢尔科夫决策过程，寻找最优策略。所谓 策略是指状态到动作的映射，策略常⽤符号π表⽰，它是指给定状态s时， 动作集上的⼀个分布，即 
$$
\pi(a|s) = p[A_{t} = a|S_{t}=s]
$$
&emsp;&emsp;策略π在每个状态s指定⼀个动作概率。如果给出的策略π是确定性的，那么策略π在每个状态s指定⼀个确定的 动作。

- **累积回报**
  强化学习是找到最优的策略，这⾥的最优是指得到的总回报最⼤。
$$
G_{t} = R_{t+1}+\gamma R_{t+2} + ... = \sum_{k=0}^{\infty }\gamma ^kR_{t+k+1}
$$
&emsp;&emsp;此时，在策略π下，利⽤上式可以计算累积回报G1 ，此时G1 有 多个可能值。由于策略π是随机的，因此累积回报也是随机的。为了评价状态s1 的价值，我们需要定义⼀个确定量来描述状态s1的价值，很⾃然的想法是利⽤累积回报来衡量状态s1 的价值。然⽽，累积回报G1是个随机变量，不是⼀个确定值，因此⽆法描述，但其期望是个确定值，可以作为状 态值函数的定义。 
- **状态值函数。**
&emsp;&emsp;当智能体采⽤策略π时，累积回报服从⼀个分布，累积回报在状态s处 的期望值定义为状态-值函数：

$$
v_{\pi}(s) = E_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}|S_{t}=s]
$$
**注意：状态值函数是与策略π相对应的，这是因为策略π决定了累积回报G的状态分布。 **