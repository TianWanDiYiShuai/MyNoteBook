# 第四章：编程流程与使用

## 1、“.pd”冻结图文件转换为UFF文件

在TensorRT的环境配置中需要注意cuda、cudnn、TensorRT的版本对应匹配关系，在最新的TensorRT5.1.2的版本中，最低支持cudnn的版本为7.5.0，cuda版本没有试过但是cuda10比较好。

对于使用的Tensorflow版本，新的Tensorflow2.0版本还不能很好的支持pd文件转uff文件，所以需要安装Tensorflow1版本。

* **获取convert-to-uff文件路径**

  ```
  # 获取which convert-to-uff路径
  # 其路径的一般位置为对于python2 /usr/lib/python2.7/dist-packages/uff/bin/convert_to_uff.py,
  # 对于python3 /usr/lib/python3.6/dist-packages/uff/bin/convert_to_uff.py,
  # 对于使用conda创建的虚拟环境：~/.conda/envs/TensorRT/bin/convert-to-uff
  which convert-to-uff
  ```

* **.pb冻结图**

  * pd冻结图的生成

    ```python
    from keras.models import load_model
    import keras.backend as K
    from tensorflow.python.framework import graph_io
    from tensorflow.python.tools import freeze_graph
    from tensorflow.core.protobuf import saver_pb2
    from tensorflow.python.training import saver as saver_lib

    def convert_keras_to_pb(keras_model, out_names, models_dir, model_filename):
        model = load_model(keras_model)
        K.set_learning_phase(0)
        sess = K.get_session()
        saver = saver_lib.Saver(write_version=saver_pb2.SaverDef.V2)
        checkpoint_path = saver.save(sess, 'saved_ckpt', global_step=0, latest_filename='checkpoint_state')
        graph_io.write_graph(sess.graph, '.', 'tmp.pb')
        freeze_graph.freeze_graph('./tmp.pb', '',
                                  False, checkpoint_path, out_names,
                                  "save/restore_all", "save/Const:0",
                                  models_dir+model_filename, False, "")
    ```

    [tensorflow冻结图官方文档](https://www.tensorflow.org/guide/extend/model_files#freezing)

    **pd转换为uff**

    ```
    convert-to-uff input_file [-o output_file] [-O output_node]
    ```

    ![](D:/个人文件/MyNoteBook/Professional_Skills/TensorRT/Image/专业技能/TensorRT/convert-to-uff.png)

  * **设置config.py文件**

  * 

## 2、通过UFF文件构建推理器

* 2.1、构建推理器对象

  ```
      // 使用TensorRT的日志对象 创建 builder
      IBuilder* builder = createInferBuilder(gLogger.getTRTLogger());
      assert(builder != nullptr);
  ```

* 2.2、创建网络

  ```
      // 为解析器创建网络
      INetworkDefinition* network = builder->createNetwork();
  ```

* 2.3、使用UFF解析器加载与解析UFF文件

  ```
     if (!parser->parse(uffFile, *network, nvinfer1::DataType::kFLOAT))
      {
          gLogError << "Failure while parsing UFF file" << std::endl;
          return nullptr;
      }
  ```

* 2.4、设置推理器的推理参数

  * 最大批量大小指定TensorRT将优化的批量大小。在运行时，可以选择较小的批量大小。
  * 层算法通常需要临时工作空间。此参数限制网络中任何层可以使用的最大大小。如果提供的scratch不足，则TensorRT可能无法找到给定层的实现。

  ```
  // 设置推理过程中最大BatchSize的大小
  builder->setMaxBatchSize(maxBatchSize);
  // 对于批量大小为5的插件层，我们需要大约1GB的划痕空间
  builder->setMaxWorkspaceSize(1_GB);
  // 设置推理的精度
      if (gArgs.runInInt8)
      {
          builder->setInt8Mode(gArgs.runInInt8);
          builder->setInt8Calibrator(calibrator);
      }    
      builder->setFp16Mode(gArgs.runInFp16);
  //
  ```

* 2.5、构建推理引擎

  ```
      ICudaEngine* engine = builder->buildCudaEngine(*network);
      if (!engine)
      {
          gLogError << "Unable to create engine" << std::endl;
          return nullptr;
      }
  ```

* 2.6、在推理引擎对象创建之后，我们不在需要，解析器和网络定义需要将其销毁

  ```
   network->destroy();
   parser->destroy();
  ```

* 2.7、序列化推理文件

  ```
   IHostMemory* trtModelStream = engine->serialize();
  ```

* 2.8、在序列化之后保存序列化后的推理文件

  ```
  // 保存方法一：
  std::ofstream serialize_output_stream("./serialize_engine_output.bin",std::ios::out | std::ios::binary);
  serialize_output_stream.write((char*)(trtModelStream->data()), trtModelStream->size());
  serialize_output_stream.close();

  // 保存方法二：
  std::stringstream gieModelStream;
  gieModelStream.seekg(0, gieModelStream.beg);
  std::ofstream outFile;
  outFile.open("./serialize_engine_output.bin");
  outFile << gieModelStream.rdbuf();
  outFile.close();

  // 保存方法三：
  std::string serialize_str;
  std::ofstream serialize_output_stream;
  serialize_str.resize(trtModelStream->size());
  memcpy((void*)serialize_str.data(), trtModelStream->data(), trtModelStream->size());
  serialize_output_stream.open("./serialize_engine_output.bin");
  serialize_output_stream << serialize_str;
  serialize_output_stream.close();
  ```

* 2.9、推理器构建之后，销毁构建器、

  ```
   builder->destroy();
   shutdownProtobufLibrary();
  ```

## 3、部署推理器

* 3.1、创建推理运行时间对象

  ```
  IRuntime* runtime = createInferRuntime(gLogger.getTRTLogger());
  ```

* 3.2、反序列化生成推理引擎

  * 3.2.1、直接通过加载内存中的序列化文件，创建推理引擎

    ​

    ```
    engine = runtime->deserializeCudaEngine(trtModelStream->data(), trtModelStream->size(), nullptr);
    assert(engine != nullptr);
    trtModelStream->destroy();
    // trtModelStream是IHostMemory*主机的内存指针对象
    ```

  * 3.2.2、通过加载plan序列化文件，创建推理引擎

    ​

    ```
    // 方法一：
    std::fstream plan_file;
    plan_file.open("./serialize_engine_output.bin", ios::binary | ios::in);
    if (!plan_file.is_open()){
        gLogInfo << "打开文件失败" << std::endl;
    }
    plan_file.seekg(0, ios::end);
    int  length = plan_file.tellg();
    plan_file.seekg(0, ios::beg);
    std::unique_ptr<char[]> data(new char[length]);
    plan_file.read(data.get(), length);

    plan_file.close();
    assert(runtime != nullptr);
    engine = runtime->deserializeCudaEngine(data.get(), length, nullptr);
    assert(engine != nullptr);

    // 方法二：
    std::string engine_s = "";
    while (plan_file.peek() != EOF) { // 使用fin.peek()防止文件读取时无限循环
    std::stringstream buffer;
    buffer << plan_file.rdbuf();
    engine_s.append(buffer.str());
    }
    plan_file.close();
    // 反序列化
    engine = runtime->deserializeCudaEngine(engine_s.data(), engine_s.size(), nullptr);
    assert(engine != nullptr);
    delete &engine_s;
    ```

* 3.3、创建执行上下文用于执行推理。

  ```
    IExecutionContext* context = engine->createExecutionContext();
    assert(context != nullptr);
  ```

* 3.4、执行推理

  ```
  // 待完成
  ```

* 3.5、删除推理器对象和执行对象

  ```
  context->destroy();
  engine->destroy();
  runtime->destroy();
  gLogger.reportTest(sampleTest, pass);
  ```

## 4、效果测试

所有测试图片格式为：3\*300\*300的ppm格式图片

* 版本：cuda-10.0 cudnn-7.5.0 TensorRT-5.1.2.2 TensorFlow-1.14.0 GPU-GTX1080 （8G）

| batchsize\params | inference time | confidence | memory use | precision\(精度\) |
| :---: | :---: | :---: | :---: | :---: |
| 1 | 5.93541 ms | 89.001 | 340M | fp16 |
| 2 | 8.97932 ms | 89.001 | 410M | fp16 |
| 3 | 10.1746 ms | 89.001 | 420M | fp16 |
| 4 | 11.8467 ms | 89.001 | 435M | fp16 |
| 5 | 14.3011 ms |  |  | fp16 |
| 6 | 16.4168 ms |  |  | fp16 |
| 7 |  |  |  |  |
| 8 |  |  |  |  |
| 9 |  |  |  |  |
| 10 |  |  |  |  |

* 版本：cuda-10.0 cudnn-7.5.0 TensorRT-5.1.2.2 TensorFlow-1.14.0 GPU-GTX1080 （8G）

| batchsize\params | inference time | confidence | memory use | precision\(精度\) |
| :---: | :---: | :---: | :---: | :---: |
| 1 | 5.93255 ms | 89.001 | 340M | fp32 |
| 2 | 8.97932 ms | 89.001 | 410M | fp32 |
| 3 | 11.0913 ms | 89.001 | 420M | fp32 |
| 4 | 12.0769 ms |  | 440M | fp32 |
| 5 | 14.7830 ms |  |  | fp32 |
| 6 | 16.4221 ms |  |  |  |
| 7 | 18.4453 ms |  |  |  |
| 8 | 19.2093 ms |  |  |  |
| 9 | 21.163 ms |  |  |  |
| 10 | 23.1207 ms |  |  |  |

**对于fp32的精度，batchsize每增加一，时间平均增加2.3ms**

## 5、出现的问题及问题的分析

### 5.1、数据类型不支持

当在运行我们的推理引擎的过程中，我们会指定其执行精度，精度可以指定为int8，fp16，fp32.但是会发现 ，在执行不同的精度的时候在不同的GPU中会报错，这是因为在不同GPU支持的数据类型有区别，对于GTX1080,RTX2080等GPU只支持32位的浮点数运算，所以使用int8类型的会报如下图的错误，并且使用fp16，和fp32的进度的效果会是 一样的，这是因为它只支持fp32的精度，不管指定的是fp16还是fp32它都 按照fp32来进行运算。

![](/Image/专业技能/TensorRT/int8报错.png)

### 5.2、输入图像数据没有进行预处理

在TensorRT中需要对推理器定义其输入和输出，在定义好其输入和输出之后，输入进推理器的数据必须按照定义的输入进行输入，在下面的SSD的例子中定义好了其输入为3\*300\*300的ppm格式图片，所以当输入不是这个类型的数据的图片时，便会报错出现指针越界的情况。

输入数据的定义：

![](/Image/专业技能/TensorRT/ssd_input.png)

输入数据不规范导致指针越界：

![](/Image/专业技能/TensorRT/input_error.png)

### 5.3、windows下的推理部署阶段的问题

windows下出现可以进行推理构建，在将构建的引擎进行运行推理。但是会出现一个现象，就是将序列化后引擎进行保存，保存为二进制的文件后，在直接通过加载这个二进制的文件，进行反序列化，然后部署推理引擎的这个过程中，会出现加载序列化文件失败的现象，要么程序直接中断，要么程序报告说为空的推理引擎指针，或者指针越界。



### 5.4、自定义层的插件问题

### 5.5、TensorRT运行精度问题



