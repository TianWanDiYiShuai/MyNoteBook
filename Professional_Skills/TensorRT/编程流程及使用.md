# 第四章：编程流程与使用

## 1、“.pd”冻结图文件转换为UFF文件

在TensorRT的环境配置中需要注意cuda、cudnn、TensorRT的版本对应匹配关系，在最新的TensorRT5.1.2的版本中，最低支持cudnn的版本为7.5.0，cuda版本没有试过但是cuda10比较好。

对于使用的Tensorflow版本，新的Tensorflow2.0版本还不能很好的支持pd文件转uff文件，所以需要安装Tensorflow1版本。

- **获取convert-to-uff文件路径**

  ```
  # 获取which convert-to-uff路径
  # 其路径的一般位置为对于python2 /usr/lib/python2.7/dist-packages/uff/bin/convert_to_uff.py,
  # 对于python3 /usr/lib/python3.6/dist-packages/uff/bin/convert_to_uff.py,
  # 对于使用conda创建的虚拟环境：~/.conda/envs/TensorRT/bin/convert-to-uff
  which convert-to-uff
  ```

- **.pb冻结图**

  - pd冻结图的生成

    ```python
    from keras.models import load_model
    import keras.backend as K
    from tensorflow.python.framework import graph_io
    from tensorflow.python.tools import freeze_graph
    from tensorflow.core.protobuf import saver_pb2
    from tensorflow.python.training import saver as saver_lib
    
    def convert_keras_to_pb(keras_model, out_names, models_dir, model_filename):
        model = load_model(keras_model)
        K.set_learning_phase(0)
        sess = K.get_session()
        saver = saver_lib.Saver(write_version=saver_pb2.SaverDef.V2)
        checkpoint_path = saver.save(sess, 'saved_ckpt', global_step=0, latest_filename='checkpoint_state')
        graph_io.write_graph(sess.graph, '.', 'tmp.pb')
        freeze_graph.freeze_graph('./tmp.pb', '',
                                  False, checkpoint_path, out_names,
                                  "save/restore_all", "save/Const:0",
                                  models_dir+model_filename, False, "")
    ```

    [tensorflow冻结图官方文档](https://www.tensorflow.org/guide/extend/model_files#freezing)

    **pd转换为uff**

    ```
    convert-to-uff input_file [-o output_file] [-O output_node]
    ```

    ![](D:/%E4%B8%AA%E4%BA%BA%E6%96%87%E4%BB%B6/MyNoteBook/Professional_Skills/TensorRT/Image/%E4%B8%93%E4%B8%9A%E6%8A%80%E8%83%BD/TensorRT/convert-to-uff.png)

  - **设置config.py文件**

  - 

## 2、通过UFF文件构建推理器

- 2.1、构建推理器对象

  ```
      // 使用TensorRT的日志对象 创建 builder
      IBuilder* builder = createInferBuilder(gLogger.getTRTLogger());
      assert(builder != nullptr);
  ```

- 2.2、创建网络

  ```
      // 为解析器创建网络
      INetworkDefinition* network = builder->createNetwork();
  ```

- 2.3、使用UFF解析器加载与解析UFF文件

  ```
     if (!parser->parse(uffFile, *network, nvinfer1::DataType::kFLOAT))
      {
          gLogError << "Failure while parsing UFF file" << std::endl;
          return nullptr;
      }
  ```

- 2.4、设置推理器的推理参数

  - 最大批量大小指定TensorRT将优化的批量大小。在运行时，可以选择较小的批量大小。
  - 层算法通常需要临时工作空间。此参数限制网络中任何层可以使用的最大大小。如果提供的scratch不足，则TensorRT可能无法找到给定层的实现。

  ```
  // 设置推理过程中最大BatchSize的大小
  builder->setMaxBatchSize(maxBatchSize);
  // 对于批量大小为5的插件层，我们需要大约1GB的划痕空间
  builder->setMaxWorkspaceSize(1_GB);
  // 设置推理的精度
      if (gArgs.runInInt8)
      {
          builder->setInt8Mode(gArgs.runInInt8);
          builder->setInt8Calibrator(calibrator);
      }    
      builder->setFp16Mode(gArgs.runInFp16);
  //
  ```

- 2.5、构建推理引擎

  ```
      ICudaEngine* engine = builder->buildCudaEngine(*network);
      if (!engine)
      {
          gLogError << "Unable to create engine" << std::endl;
          return nullptr;
      }
  ```

- 2.6、在推理引擎对象创建之后，我们不在需要，解析器和网络定义需要将其销毁

  ```
   network->destroy();
   parser->destroy();
  ```

- 2.7、序列化推理文件

  ```
   IHostMemory* trtModelStream = engine->serialize();
  ```

- 2.8、在序列化之后保存序列化后的推理文件

  ```
  // 保存方法一：
  std::ofstream serialize_output_stream("./serialize_engine_output.bin",std::ios::out | std::ios::binary);
  serialize_output_stream.write((char*)(trtModelStream->data()), trtModelStream->size());
  serialize_output_stream.close();
  
  // 保存方法二：
  std::stringstream gieModelStream;
  gieModelStream.seekg(0, gieModelStream.beg);
  std::ofstream outFile;
  outFile.open("./serialize_engine_output.bin");
  outFile << gieModelStream.rdbuf();
  outFile.close();
  
  // 保存方法三：
  std::string serialize_str;
  std::ofstream serialize_output_stream;
  serialize_str.resize(trtModelStream->size());
  memcpy((void*)serialize_str.data(), trtModelStream->data(), trtModelStream->size());
  serialize_output_stream.open("./serialize_engine_output.bin");
  serialize_output_stream << serialize_str;
  serialize_output_stream.close();
  ```

- 2.9、推理器构建之后，销毁构建器、

  ```
   builder->destroy();
   shutdownProtobufLibrary();
  ```

## 3、部署推理器

- 3.1、创建推理运行时间对象

  ```
  IRuntime* runtime = createInferRuntime(gLogger.getTRTLogger());
  ```

- 3.2、反序列化生成推理引擎

  - 3.2.1、直接通过加载内存中的序列化文件，创建推理引擎

    ​

    ```
    engine = runtime->deserializeCudaEngine(trtModelStream->data(), trtModelStream->size(), nullptr);
    assert(engine != nullptr);
    trtModelStream->destroy();
    // trtModelStream是IHostMemory*主机的内存指针对象
    ```

  - 3.2.2、通过加载plan序列化文件，创建推理引擎

    ​

    ```
    // 方法一：
    std::fstream plan_file;
    plan_file.open("./serialize_engine_output.bin", ios::binary | ios::in);
    if (!plan_file.is_open()){
        gLogInfo << "打开文件失败" << std::endl;
    }
    plan_file.seekg(0, ios::end);
    int  length = plan_file.tellg();
    plan_file.seekg(0, ios::beg);
    std::unique_ptr<char[]> data(new char[length]);
    plan_file.read(data.get(), length);
    
    plan_file.close();
    assert(runtime != nullptr);
    engine = runtime->deserializeCudaEngine(data.get(), length, nullptr);
    assert(engine != nullptr);
    
    // 方法二：
    std::string engine_s = "";
    while (plan_file.peek() != EOF) { // 使用fin.peek()防止文件读取时无限循环
    std::stringstream buffer;
    buffer << plan_file.rdbuf();
    engine_s.append(buffer.str());
    }
    plan_file.close();
    // 反序列化
    engine = runtime->deserializeCudaEngine(engine_s.data(), engine_s.size(), nullptr);
    assert(engine != nullptr);
    delete &engine_s;
    ```

- 3.3、创建执行上下文用于执行推理。

  ```
    IExecutionContext* context = engine->createExecutionContext();
    assert(context != nullptr);
  ```

- 3.4、执行推理

  ```
  // 待完成
  ```

- 3.5、删除推理器对象和执行对象

  ```
  context->destroy();
  engine->destroy();
  runtime->destroy();
  gLogger.reportTest(sampleTest, pass);
  ```

## 4、效果测试

所有测试图片格式为：3\*300\*300的ppm格式图片

- 版本：cuda-10.0 cudnn-7.5.0 TensorRT-5.1.2.2 TensorFlow-1.14.0 GPU-GTX1080 （8G）

| batchsize\params | inference time | confidence | memory use | precision\(精度\) |
| :--------------: | :------------: | :--------: | :--------: | :---------------: |
|        1         |   5.93541 ms   |   89.001   |    340M    |       fp16        |
|        2         |   8.97932 ms   |   89.001   |    410M    |       fp16        |
|        3         |   10.1746 ms   |   89.001   |    420M    |       fp16        |
|        4         |   11.8467 ms   |   89.001   |    435M    |       fp16        |
|        5         |   14.3011 ms   |            |            |       fp16        |
|        6         |   16.4168 ms   |            |            |       fp16        |
|        7         |                |            |            |                   |
|        8         |                |            |            |                   |
|        9         |                |            |            |                   |
|        10        |                |            |            |                   |

- 版本：cuda-10.0 cudnn-7.5.0 TensorRT-5.1.2.2 TensorFlow-1.14.0 GPU-GTX1080 （8G）

| batchsize\params | inference time | confidence | memory use | precision\(精度\) |
| :--------------: | :------------: | :--------: | :--------: | :---------------: |
|        1         |   5.93255 ms   |   89.001   |    340M    |       fp32        |
|        2         |   8.97932 ms   |   89.001   |    410M    |       fp32        |
|        3         |   11.0913 ms   |   89.001   |    420M    |       fp32        |
|        4         |   12.0769 ms   |            |    440M    |       fp32        |
|        5         |   14.7830 ms   |            |            |       fp32        |
|        6         |   16.4221 ms   |            |            |                   |
|        7         |   18.4453 ms   |            |            |                   |
|        8         |   19.2093 ms   |            |            |                   |
|        9         |   21.163 ms    |            |            |                   |
|        10        |   23.1207 ms   |            |            |                   |

**对于fp32的精度，batchsize每增加一，时间平均增加2.3ms**

## 5、出现的问题及问题的分析

### 5.1、数据类型不支持

当在运行我们的推理引擎的过程中，我们会指定其执行精度，精度可以指定为int8，fp16，fp32.但是会发现 ，在执行不同的精度的时候在不同的GPU中会报错，这是因为在不同GPU支持的数据类型有区别，对于GTX1080,RTX2080等GPU只支持32位的浮点数运算，所以使用int8类型的会报如下图的错误，并且使用fp16，和fp32的进度的效果会是 一样的，这是因为它只支持fp32的精度，不管指定的是fp16还是fp32它都 按照fp32来进行运算。

### 5.2、输入图像数据没有进行预处理

  在TensorRT中需要对推理器定义其输入和输出，在定义好其输入和输出之后，输入进推理器的数据必须按照定义的输入进行输入，在下面的SSD的例子中定义好了其输入为3\*300\*300的ppm格式图片，所以当输入不是这个类型的数据的图片时，便会报错出现指针越界的情况。

输入数据的定义：



输入数据不规范导致指针越界：

### 5.3、windows下的推理部署阶段的问题

windows下出现可以进行推理构建，在将构建的引擎进行运行推理。但是会出现一个现象，就是将序列化后引擎进行保存，保存为二进制的文件后，在直接通过加载这个二进制的文件，进行反序列化，然后部署推理引擎的这个过程中，会出现加载序列化文件失败的现象，要么程序直接中断，要么程序报告说为空的推理引擎指针，或者指针越界。

### 5.4、自定义层的插件问题



### 5.5、TensorRT运行精度问题



