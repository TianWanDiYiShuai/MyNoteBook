# 第四章：编程流程与使用

## 1、“.pd”冻结图文件转换为UFF文件

在TensorRT的环境配置中需要注意cuda、cudnn、TensorRT的版本对应匹配关系，在最新的TensorRT5.1.2的版本中，最低支持cudnn的版本为7.5.0，cuda版本没有试过但是cuda10比较好。

对于使用的Tensorflow版本，新的Tensorflow2.0版本还不能很好的支持pd文件转uff文件，所以需要安装Tensorflow1版本。

* **获取convert-to-uff文件路径**

  ```
  # 获取which convert-to-uff路径
  # 其路径的一般位置为对于python2 /usr/lib/python2.7/dist-packages/uff/bin/convert_to_uff.py,
  # 对于python3 /usr/lib/python3.6/dist-packages/uff/bin/convert_to_uff.py,
  # 对于使用conda创建的虚拟环境：~/.conda/envs/TensorRT/bin/convert-to-uff
  which convert-to-uff
  ```

* **.pb冻结图**

  * pd冻结图的生成

    ```python
    from keras.models import load_model
    import keras.backend as K
    from tensorflow.python.framework import graph_io
    from tensorflow.python.tools import freeze_graph
    from tensorflow.core.protobuf import saver_pb2
    from tensorflow.python.training import saver as saver_lib

    def convert_keras_to_pb(keras_model, out_names, models_dir, model_filename):
        model = load_model(keras_model)
        K.set_learning_phase(0)
        sess = K.get_session()
        saver = saver_lib.Saver(write_version=saver_pb2.SaverDef.V2)
        checkpoint_path = saver.save(sess, 'saved_ckpt', global_step=0, latest_filename='checkpoint_state')
        graph_io.write_graph(sess.graph, '.', 'tmp.pb')
        freeze_graph.freeze_graph('./tmp.pb', '',
                                  False, checkpoint_path, out_names,
                                  "save/restore_all", "save/Const:0",
                                  models_dir+model_filename, False, "")
    ```

    [tensorflow冻结图官方文档](https://www.tensorflow.org/guide/extend/model_files#freezing)

    **pd转换为uff**

    ```
    convert-to-uff input_file [-o output_file] [-O output_node]
    ```

    ![](/Image/专业技能/TensorRT/convert-to-uff.png)

  * **设置config.py文件**

  * 

## 2、通过UFF文件构建推理器

* 2.1、构建推理器对象

  ```
      // 使用TensorRT的日志对象 创建 builder
      IBuilder* builder = createInferBuilder(gLogger.getTRTLogger());
      assert(builder != nullptr);
  ```

* 2.2、创建网络

  ```
      // 为解析器创建网络
      INetworkDefinition* network = builder->createNetwork();
  ```

* 2.3、使用UFF解析器加载与解析UFF文件

  ```
     if (!parser->parse(uffFile, *network, nvinfer1::DataType::kFLOAT))
      {
          gLogError << "Failure while parsing UFF file" << std::endl;
          return nullptr;
      }
  ```

* 2.4、设置推理器的推理参数

  * 最大批量大小指定TensorRT将优化的批量大小。在运行时，可以选择较小的批量大小。
  * 层算法通常需要临时工作空间。此参数限制网络中任何层可以使用的最大大小。如果提供的scratch不足，则TensorRT可能无法找到给定层的实现。

  ```
  // 设置推理过程中最大BatchSize的大小
  builder->setMaxBatchSize(maxBatchSize);
  // 对于批量大小为5的插件层，我们需要大约1GB的划痕空间
  builder->setMaxWorkspaceSize(1_GB);
  // 设置推理的精度
      if (gArgs.runInInt8)
      {
          builder->setInt8Mode(gArgs.runInInt8);
          builder->setInt8Calibrator(calibrator);
      }    
      builder->setFp16Mode(gArgs.runInFp16);
  //
  ```

* 2.5、构建推理引擎

  ```
      ICudaEngine* engine = builder->buildCudaEngine(*network);
      if (!engine)
      {
          gLogError << "Unable to create engine" << std::endl;
          return nullptr;
      }
  ```

* 2.6、在推理引擎对象创建之后，我们不在需要，解析器和网络定义需要将其销毁

  ```
   network->destroy();
   parser->destroy();
  ```

* 2.7、序列化推理文件

  ```
   IHostMemory* trtModelStream = engine->serialize();
  ```

* 2.8、在序列化之后保存序列化后的推理文件

  ```
  // 保存方法一：
  std::ofstream serialize_output_stream("./serialize_engine_output.bin",std::ios::out | std::ios::binary);
  serialize_output_stream.write((char*)(trtModelStream->data()), trtModelStream->size());
  serialize_output_stream.close();

  // 保存方法二：
  std::stringstream gieModelStream;
  gieModelStream.seekg(0, gieModelStream.beg);
  std::ofstream outFile;
  outFile.open("./serialize_engine_output.bin");
  outFile << gieModelStream.rdbuf();
  outFile.close();

  // 保存方法三：
  std::string serialize_str;
  std::ofstream serialize_output_stream;
  serialize_str.resize(trtModelStream->size());
  memcpy((void*)serialize_str.data(), trtModelStream->data(), trtModelStream->size());
  serialize_output_stream.open("./serialize_engine_output.bin");
  serialize_output_stream << serialize_str;
  serialize_output_stream.close();
  ```

* 2.9、推理器构建之后，销毁构建器、

  ```
   builder->destroy();
   shutdownProtobufLibrary();
  ```

## 3、部署推理器

* 3.1、创建推理运行时间对象

  ```
  IRuntime* runtime = createInferRuntime(gLogger.getTRTLogger());
  ```

* 3.2、反序列化生成推理引擎

  * 3.2.1、直接通过加载内存中的序列化文件，创建推理引擎

    ​

    ```
    engine = runtime->deserializeCudaEngine(trtModelStream->data(), trtModelStream->size(), nullptr);
    assert(engine != nullptr);
    trtModelStream->destroy();
    // trtModelStream是IHostMemory*主机的内存指针对象
    ```

  * 3.2.2、通过加载plan序列化文件，创建推理引擎

    ​

    ```
    // 方法一：
    std::fstream plan_file;
    plan_file.open("./serialize_engine_output.bin", ios::binary | ios::in);
    if (!plan_file.is_open()){
        gLogInfo << "打开文件失败" << std::endl;
    }
    plan_file.seekg(0, ios::end);
    int  length = plan_file.tellg();
    plan_file.seekg(0, ios::beg);
    std::unique_ptr<char[]> data(new char[length]);
    plan_file.read(data.get(), length);

    plan_file.close();
    assert(runtime != nullptr);
    engine = runtime->deserializeCudaEngine(data.get(), length, nullptr);
    assert(engine != nullptr);

    // 方法二：
    std::string engine_s = "";
    while (plan_file.peek() != EOF) { // 使用fin.peek()防止文件读取时无限循环
    std::stringstream buffer;
    buffer << plan_file.rdbuf();
    engine_s.append(buffer.str());
    }
    plan_file.close();
    // 反序列化
    engine = runtime->deserializeCudaEngine(engine_s.data(), engine_s.size(), nullptr);
    assert(engine != nullptr);
    delete &engine_s;
    ```

* 3.3、创建执行上下文用于执行推理。

  ```
    IExecutionContext* context = engine->createExecutionContext();
    assert(context != nullptr);
  ```

* 3.4、执行推理

  ```
  // 待完成
  ```

* 3.5、删除推理器对象和执行对象

  ```
  context->destroy();
  engine->destroy();
  runtime->destroy();
  gLogger.reportTest(sampleTest, pass);
  ```

## 4、效果测试

| batches\params | inference time | confidence | memory use | precision\(精度\) |
| :---: | :---: | :---: | :---: | :---: |
|  |  |  |  |  |
|  |  |  |  |  |



